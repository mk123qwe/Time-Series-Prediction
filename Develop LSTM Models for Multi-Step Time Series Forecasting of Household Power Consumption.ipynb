{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/how-to-develop-lstm-models-for-multi-step-time-series-forecasting-of-household-power-consumption/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and clean-up data\n",
    "from numpy import nan\n",
    "from numpy import isnan\n",
    "from pandas import read_csv\n",
    "from pandas import to_numeric\n",
    "\n",
    "# fill missing values with a value at the same time one day ago\n",
    "def fill_missing(values):\n",
    "\tone_day = 60 * 24\n",
    "\tfor row in range(values.shape[0]):\n",
    "\t\tfor col in range(values.shape[1]):\n",
    "\t\t\tif isnan(values[row, col]):\n",
    "\t\t\t\tvalues[row, col] = values[row - one_day, col]\n",
    "\n",
    "# load all data\n",
    "dataset = read_csv('../data/household_power_consumption.txt', sep=';', header=0, low_memory=False, infer_datetime_format=True, parse_dates={'datetime':[0,1]}, index_col=['datetime'])\n",
    "# mark all missing values\n",
    "dataset.replace('?', nan, inplace=True)\n",
    "# make dataset numeric\n",
    "dataset = dataset.astype('float32')\n",
    "# fill missing\n",
    "fill_missing(dataset.values)\n",
    "# add a column for for the remainder of sub metering\n",
    "values = dataset.values\n",
    "dataset['sub_metering_4'] = (values[:,0] * 1000 / 60) - (values[:,4] + values[:,5] + values[:,6])\n",
    "# save updated dataset\n",
    "dataset.to_csv('../data/household_power_consumption.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample minute data to total for each day\n",
    "from pandas import read_csv\n",
    "# load the new file\n",
    "dataset = read_csv('../data/household_power_consumption.csv', header=0, infer_datetime_format=True, parse_dates=['datetime'], index_col=['datetime'])\n",
    "# resample data to daily\n",
    "daily_groups = dataset.resample('D')\n",
    "daily_data = daily_groups.sum()\n",
    "# summarize\n",
    "print(daily_data.shape)\n",
    "print(daily_data.head())\n",
    "# save\n",
    "daily_data.to_csv('../data/household_power_consumption_days.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into standard weeks\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "\t# split into standard weeks\n",
    "\ttrain, test = data[1:-328], data[-328:-6]\n",
    "\t# restructure into windows of weekly data\n",
    "\ttrain = array(split(train, len(train)/7))\n",
    "\ttest = array(split(test, len(test)/7))\n",
    "\treturn train, test\n",
    "\n",
    "# load the new file\n",
    "dataset = read_csv('../data/household_power_consumption_days.csv', header=0, infer_datetime_format=True, parse_dates=['datetime'], index_col=['datetime'])\n",
    "train, test = split_dataset(dataset.values)\n",
    "# validate train data\n",
    "print(train.shape)\n",
    "print(train[0, 0, 0], train[-1, -1, 0])\n",
    "# validate test\n",
    "print(test.shape)\n",
    "print(test[0, 0, 0], test[-1, -1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = dataset.values[-328:-6]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t)/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每7行一个数组\n",
    "split(t, len(t)/7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多个2D转3D数组\n",
    "test1 = array(split(t, len(t)/7))\n",
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate multi-step lstm\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "\t# split into standard weeks\n",
    "\ttrain, test = data[1:-328], data[-328:-6]\n",
    "\t# restructure into windows of weekly data\n",
    "\ttrain = array(split(train, len(train)/7))\n",
    "\ttest = array(split(test, len(test)/7))\n",
    "\treturn train, test\n",
    "\n",
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "\tscores = list()\n",
    "\t# calculate an RMSE score for each day\n",
    "\tfor i in range(actual.shape[1]):\n",
    "\t\t# calculate mse\n",
    "\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "\t\t# calculate rmse\n",
    "\t\trmse = sqrt(mse)\n",
    "\t\t# store\n",
    "\t\tscores.append(rmse)\n",
    "\t# calculate overall RMSE\n",
    "\ts = 0\n",
    "\tfor row in range(actual.shape[0]):\n",
    "\t\tfor col in range(actual.shape[1]):\n",
    "\t\t\ts += (actual[row, col] - predicted[row, col])**2\n",
    "\tscore = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "\treturn score, scores\n",
    "\n",
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "\ts_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "\tprint('%s: [%.3f] %s' % (name, score, s_scores))\n",
    "\n",
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=7):\n",
    "\t# flatten data\n",
    "\tdata = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "\tX, y = list(), list()\n",
    "\tin_start = 0\n",
    "\t# step over the entire history one time step at a time\n",
    "\tfor _ in range(len(data)):\n",
    "\t\t# define the end of the input sequence\n",
    "\t\tin_end = in_start + n_input\n",
    "\t\tout_end = in_end + n_out\n",
    "\t\t# ensure we have enough data for this instance\n",
    "\t\tif out_end < len(data):\n",
    "\t\t\tx_input = data[in_start:in_end, 0]\n",
    "\t\t\tx_input = x_input.reshape((len(x_input), 1))\n",
    "\t\t\tX.append(x_input)\n",
    "\t\t\ty.append(data[in_end:out_end, 0])\n",
    "\t\t# move along one time step\n",
    "\t\tin_start += 1\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# train the model\n",
    "def build_model(train, n_input):\n",
    "\t# prepare data\n",
    "\ttrain_x, train_y = to_supervised(train, n_input)\n",
    "\t# define parameters\n",
    "\tverbose, epochs, batch_size = 1, 70, 16\n",
    "\tn_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "\tmodel.add(Dense(100, activation='relu'))\n",
    "\tmodel.add(Dense(n_outputs))\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\t# fit network\n",
    "\tmodel.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\treturn model\n",
    "\n",
    "# make a forecast\n",
    "def forecast(model, history, n_input):\n",
    "\t# flatten data\n",
    "\tdata = array(history)\n",
    "\tdata = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "\t# retrieve last observations for input data，读取每次添加的数组\n",
    "\tinput_x = data[-n_input:, 0]\n",
    "\t# reshape into [1, n_input, 1]\n",
    "\tinput_x = input_x.reshape((1, len(input_x), 1))\n",
    "\t# forecast the next week\n",
    "\tyhat = model.predict(input_x, verbose=0)\n",
    "\t# we only want the vector forecast\n",
    "\tyhat = yhat[0]\n",
    "\treturn yhat\n",
    "\n",
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_input):\n",
    "\t# fit model\n",
    "\tmodel = build_model(train, n_input)\n",
    "\t# history is a list of weekly data\n",
    "\thistory = [x for x in train]\n",
    "\t# walk-forward validation over each week\n",
    "\tpredictions = list()\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# predict the week\n",
    "\t\tyhat_sequence = forecast(model, history, n_input)\n",
    "\t\t# store the predictions\n",
    "\t\tpredictions.append(yhat_sequence)\n",
    "\t\t# get real observation and add to history for predicting the next week\n",
    "\t\thistory.append(test[i, :])\n",
    "\t# evaluate predictions days for each week\n",
    "\tpredictions = array(predictions)\n",
    "\tscore, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "\treturn score, scores\n",
    "\n",
    "# load the new file\n",
    "dataset = read_csv('../data/household_power_consumption_days.csv', header=0, infer_datetime_format=True, parse_dates=['datetime'], index_col=['datetime'])\n",
    "# split into train and test\n",
    "train, test = split_dataset(dataset.values)\n",
    "# evaluate model and get scores\n",
    "n_input = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = to_supervised(train, n_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每次读取一个7行的2D数组\n",
    "history = [x for x in train]\n",
    "#print(history)\n",
    "print(history[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten data\n",
    "data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "print(data)\n",
    "print(data.shape)\n",
    "#159*7=1113行；8列\n",
    "print(train.shape[0])\n",
    "print(train.shape[1])\n",
    "print(train.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_supervised(train, n_input, n_out=7):\n",
    "\t# flatten data\n",
    "\tdata = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "\tX, y = list(), list()\n",
    "\tin_start = 0\n",
    "\t# step over the entire history one time step at a time\n",
    "\tfor _ in range(len(data)):\n",
    "\t\t# define the end of the input sequence\n",
    "\t\tin_end = in_start + n_input\n",
    "\t\tout_end = in_end + n_out\n",
    "\t\t# ensure we have enough data for this instance\n",
    "\t\tif out_end < len(data):\n",
    "\t\t\tx_input = data[in_start:in_end, 0]\n",
    "\t\t\tx_input = x_input.reshape((len(x_input), 1))\n",
    "\t\t\tX.append(x_input)\n",
    "\t\t\ty.append(data[in_end:out_end, 0])\n",
    "\t\t# move along one time step\n",
    "\t\tin_start += 1\n",
    "\treturn array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_dataset(dataset.values)\n",
    "# evaluate model and get scores\n",
    "n_input = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = to_supervised(train, 7)\n",
    "#前一个7天\n",
    "print(train_x[0])\n",
    "#后一个7天\n",
    "print(train_y[0])\n",
    "print(train_x.shape[1])\n",
    "print(train_x.shape[2])\n",
    "print(train_y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每次读取一个7行的2D数组\n",
    "history = [x for x in train]\n",
    "#print(history)\n",
    "print(history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data1 = array(history)\n",
    "data1 = data1.reshape((data1.shape[0]*data1.shape[1], data1.shape[2]))\n",
    "\n",
    "\n",
    "# retrieve last observations for input data\n",
    "input_x = data1[-n_input:, 0]\n",
    "print(input_x)\n",
    "print(data1[-1, 0])\n",
    "print(train[-1, 0])\n",
    "print(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reshape into [1, n_input, 1]\n",
    "input_x = input_x.reshape((1, len(input_x), 1))\n",
    "print(input_x)\n",
    "# forecast the next week\n",
    "yhat = model.predict(input_x, verbose=0)\n",
    "print(yhat)\n",
    "# we only want the vector forecast\n",
    "yhat = yhat[0]\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list()\n",
    "for i in range(len(test)):\n",
    "# predict the week\n",
    "\t\tyhat_sequence = forecast(model, history, n_input)\n",
    "\t\t# store the predictions\n",
    "\t\tpredictions.append(yhat_sequence)\n",
    "\t\t# get real observation and add to history for predicting the next week\n",
    "\t\thistory.append(test[i, :])\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每次添加一个7天2D数组\n",
    "test[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = array(predictions)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, scores = evaluate_model(train, test, n_input)\n",
    "# summarize scores\n",
    "summarize_scores('lstm', score, scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(train, n_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[0,0]\n",
    "len(test[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scores\n",
    "days = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\n",
    "pyplot.plot(days, scores, marker='o', label='lstm')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = [419.4, 422.1, 384.5, 395.1, 403.9, 317.7, 441.5]\n",
    "np.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate multi-step encoder-decoder lstm\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "\t# split into standard weeks\n",
    "\ttrain, test = data[1:-328], data[-328:-6]\n",
    "\t# restructure into windows of weekly data\n",
    "\ttrain = array(split(train, len(train)/7))\n",
    "\ttest = array(split(test, len(test)/7))\n",
    "\treturn train, test\n",
    "\n",
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "\tscores = list()\n",
    "\t# calculate an RMSE score for each day\n",
    "\tfor i in range(actual.shape[1]):\n",
    "\t\t# calculate mse\n",
    "\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "\t\t# calculate rmse\n",
    "\t\trmse = sqrt(mse)\n",
    "\t\t# store\n",
    "\t\tscores.append(rmse)\n",
    "\t# calculate overall RMSE\n",
    "\ts = 0\n",
    "\tfor row in range(actual.shape[0]):\n",
    "\t\tfor col in range(actual.shape[1]):\n",
    "\t\t\ts += (actual[row, col] - predicted[row, col])**2\n",
    "\tscore = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "\treturn score, scores\n",
    "\n",
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "\ts_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "\tprint('%s: [%.3f] %s' % (name, score, s_scores))\n",
    "\n",
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=7):\n",
    "\t# flatten data\n",
    "\tdata = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "\tX, y = list(), list()\n",
    "\tin_start = 0\n",
    "\t# step over the entire history one time step at a time\n",
    "\tfor _ in range(len(data)):\n",
    "\t\t# define the end of the input sequence\n",
    "\t\tin_end = in_start + n_input\n",
    "\t\tout_end = in_end + n_out\n",
    "\t\t# ensure we have enough data for this instance\n",
    "\t\tif out_end < len(data):\n",
    "\t\t\tx_input = data[in_start:in_end, 0]\n",
    "\t\t\tx_input = x_input.reshape((len(x_input), 1))\n",
    "\t\t\tX.append(x_input)\n",
    "\t\t\ty.append(data[in_end:out_end, 0])\n",
    "\t\t# move along one time step\n",
    "\t\tin_start += 1\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# train the model\n",
    "def build_model(train, n_input):\n",
    "\t# prepare data\n",
    "\ttrain_x, train_y = to_supervised(train, n_input)\n",
    "\t# define parameters\n",
    "\tverbose, epochs, batch_size = 0, 20, 16\n",
    "\tn_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "\t# reshape output into [samples, timesteps, features]\n",
    "\ttrain_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "\tmodel.add(RepeatVector(n_outputs))\n",
    "\tmodel.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "\tmodel.add(TimeDistributed(Dense(1)))\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\t# fit network\n",
    "\tmodel.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\treturn model\n",
    "\n",
    "# make a forecast\n",
    "def forecast(model, history, n_input):\n",
    "\t# flatten data\n",
    "\tdata = array(history)\n",
    "\tdata = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "\t# retrieve last observations for input data\n",
    "\tinput_x = data[-n_input:, 0]\n",
    "\t# reshape into [1, n_input, 1]\n",
    "\tinput_x = input_x.reshape((1, len(input_x), 1))\n",
    "\t# forecast the next week\n",
    "\tyhat = model.predict(input_x, verbose=1)\n",
    "\t# we only want the vector forecast\n",
    "\tyhat = yhat[0]\n",
    "\treturn yhat\n",
    "\n",
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_input):\n",
    "\t# fit model\n",
    "\tmodel = build_model(train, n_input)\n",
    "\t# history is a list of weekly data\n",
    "\thistory = [x for x in train]\n",
    "\t# walk-forward validation over each week\n",
    "\tpredictions = list()\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# predict the week\n",
    "\t\tyhat_sequence = forecast(model, history, n_input)\n",
    "\t\t# store the predictions\n",
    "\t\tpredictions.append(yhat_sequence)\n",
    "\t\t# get real observation and add to history for predicting the next week\n",
    "\t\thistory.append(test[i, :])\n",
    "\t# evaluate predictions days for each week\n",
    "\tpredictions = array(predictions)\n",
    "\tscore, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "\treturn score, scores\n",
    "\n",
    "# load the new file\n",
    "dataset = read_csv('../data/household_power_consumption_days.csv', header=0, infer_datetime_format=True, parse_dates=['datetime'], index_col=['datetime'])\n",
    "# split into train and test\n",
    "train, test = split_dataset(dataset.values)\n",
    "# evaluate model and get scores\n",
    "n_input = 14\n",
    "score, scores = evaluate_model(train, test, n_input)\n",
    "# summarize scores\n",
    "summarize_scores('lstm', score, scores)\n",
    "# plot scores\n",
    "days = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\n",
    "pyplot.plot(days, scores, marker='o', label='lstm')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate multi-step encoder-decoder lstm\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "\t# split into standard weeks\n",
    "\ttrain, test = data[1:-328], data[-328:-6]\n",
    "\t# restructure into windows of weekly data\n",
    "\ttrain = array(split(train, len(train)/7))\n",
    "\ttest = array(split(test, len(test)/7))\n",
    "\treturn train, test\n",
    "\n",
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "\tscores = list()\n",
    "\t# calculate an RMSE score for each day\n",
    "\tfor i in range(actual.shape[1]):\n",
    "\t\t# calculate mse\n",
    "\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "\t\t# calculate rmse\n",
    "\t\trmse = sqrt(mse)\n",
    "\t\t# store\n",
    "\t\tscores.append(rmse)\n",
    "\t# calculate overall RMSE\n",
    "\ts = 0\n",
    "\tfor row in range(actual.shape[0]):\n",
    "\t\tfor col in range(actual.shape[1]):\n",
    "\t\t\ts += (actual[row, col] - predicted[row, col])**2\n",
    "\tscore = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "\treturn score, scores\n",
    "\n",
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "\ts_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "\tprint('%s: [%.3f] %s' % (name, score, s_scores))\n",
    "\n",
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=7):\n",
    "\t# flatten data\n",
    "\tdata = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "\tX, y = list(), list()\n",
    "\tin_start = 0\n",
    "\t# step over the entire history one time step at a time\n",
    "\tfor _ in range(len(data)):\n",
    "\t\t# define the end of the input sequence\n",
    "\t\tin_end = in_start + n_input\n",
    "\t\tout_end = in_end + n_out\n",
    "\t\t# ensure we have enough data for this instance\n",
    "\t\tif out_end < len(data):\n",
    "\t\t\tX.append(data[in_start:in_end, :])\n",
    "\t\t\ty.append(data[in_end:out_end, 0])\n",
    "\t\t# move along one time step\n",
    "\t\tin_start += 1\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# train the model\n",
    "def build_model(train, n_input):\n",
    "\t# prepare data\n",
    "\ttrain_x, train_y = to_supervised(train, n_input)\n",
    "\t# define parameters\n",
    "\tverbose, epochs, batch_size = 0, 50, 16\n",
    "\tn_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "\t# reshape output into [samples, timesteps, features]\n",
    "\ttrain_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "\tmodel.add(RepeatVector(n_outputs))\n",
    "\tmodel.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "\tmodel.add(TimeDistributed(Dense(1)))\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\t# fit network\n",
    "\tmodel.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\treturn model\n",
    "\n",
    "# make a forecast\n",
    "def forecast(model, history, n_input):\n",
    "\t# flatten data\n",
    "\tdata = array(history)\n",
    "\tdata = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "\t# retrieve last observations for input data\n",
    "\tinput_x = data[-n_input:, :]\n",
    "\t# reshape into [1, n_input, n]\n",
    "\tinput_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
    "\t# forecast the next week\n",
    "\tyhat = model.predict(input_x, verbose=1)\n",
    "\t# we only want the vector forecast\n",
    "\tyhat = yhat[0]\n",
    "\treturn yhat\n",
    "\n",
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_input):\n",
    "\t# fit model\n",
    "\tmodel = build_model(train, n_input)\n",
    "\t# history is a list of weekly data\n",
    "\thistory = [x for x in train]\n",
    "\t# walk-forward validation over each week\n",
    "\tpredictions = list()\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# predict the week\n",
    "\t\tyhat_sequence = forecast(model, history, n_input)\n",
    "\t\t# store the predictions\n",
    "\t\tpredictions.append(yhat_sequence)\n",
    "\t\t# get real observation and add to history for predicting the next week\n",
    "\t\thistory.append(test[i, :])\n",
    "\t# evaluate predictions days for each week\n",
    "\tpredictions = array(predictions)\n",
    "\tscore, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "\treturn score, scores\n",
    "\n",
    "# load the new file\n",
    "dataset = read_csv('../data/household_power_consumption_days.csv', header=0, infer_datetime_format=True, parse_dates=['datetime'], index_col=['datetime'])\n",
    "# split into train and test\n",
    "train, test = split_dataset(dataset.values)\n",
    "# evaluate model and get scores\n",
    "n_input = 14\n",
    "score, scores = evaluate_model(train, test, n_input)\n",
    "# summarize scores\n",
    "summarize_scores('lstm', score, scores)\n",
    "# plot scores\n",
    "days = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\n",
    "pyplot.plot(days, scores, marker='o', label='lstm')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate multi-step encoder-decoder cnn-lstm\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "\t# split into standard weeks\n",
    "\ttrain, test = data[1:-328], data[-328:-6]\n",
    "\t# restructure into windows of weekly data\n",
    "\ttrain = array(split(train, len(train)/7))\n",
    "\ttest = array(split(test, len(test)/7))\n",
    "\treturn train, test\n",
    "\n",
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "\tscores = list()\n",
    "\t# calculate an RMSE score for each day\n",
    "\tfor i in range(actual.shape[1]):\n",
    "\t\t# calculate mse\n",
    "\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "\t\t# calculate rmse\n",
    "\t\trmse = sqrt(mse)\n",
    "\t\t# store\n",
    "\t\tscores.append(rmse)\n",
    "\t# calculate overall RMSE\n",
    "\ts = 0\n",
    "\tfor row in range(actual.shape[0]):\n",
    "\t\tfor col in range(actual.shape[1]):\n",
    "\t\t\ts += (actual[row, col] - predicted[row, col])**2\n",
    "\tscore = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "\treturn score, scores\n",
    "\n",
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "\ts_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "\tprint('%s: [%.3f] %s' % (name, score, s_scores))\n",
    "\n",
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=7):\n",
    "\t# flatten data\n",
    "\tdata = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "\tX, y = list(), list()\n",
    "\tin_start = 0\n",
    "\t# step over the entire history one time step at a time\n",
    "\tfor _ in range(len(data)):\n",
    "\t\t# define the end of the input sequence\n",
    "\t\tin_end = in_start + n_input\n",
    "\t\tout_end = in_end + n_out\n",
    "\t\t# ensure we have enough data for this instance\n",
    "\t\tif out_end < len(data):\n",
    "\t\t\tx_input = data[in_start:in_end, 0]\n",
    "\t\t\tx_input = x_input.reshape((len(x_input), 1))\n",
    "\t\t\tX.append(x_input)\n",
    "\t\t\ty.append(data[in_end:out_end, 0])\n",
    "\t\t# move along one time step\n",
    "\t\tin_start += 1\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# train the model\n",
    "def build_model(train, n_input):\n",
    "\t# prepare data\n",
    "\ttrain_x, train_y = to_supervised(train, n_input)\n",
    "\t# define parameters\n",
    "\tverbose, epochs, batch_size = 0, 20, 16\n",
    "\tn_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "\t# reshape output into [samples, timesteps, features]\n",
    "\ttrain_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "\tmodel.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "\tmodel.add(MaxPooling1D(pool_size=2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(RepeatVector(n_outputs))\n",
    "\tmodel.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "\tmodel.add(TimeDistributed(Dense(1)))\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\t# fit network\n",
    "\tmodel.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\treturn model\n",
    "\n",
    "# make a forecast\n",
    "def forecast(model, history, n_input):\n",
    "\t# flatten data\n",
    "\tdata = array(history)\n",
    "\tdata = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "\t# retrieve last observations for input data\n",
    "\tinput_x = data[-n_input:, 0]\n",
    "\t# reshape into [1, n_input, 1]\n",
    "\tinput_x = input_x.reshape((1, len(input_x), 1))\n",
    "\t# forecast the next week\n",
    "\tyhat = model.predict(input_x, verbose=1)\n",
    "\t# we only want the vector forecast\n",
    "\tyhat = yhat[0]\n",
    "\treturn yhat\n",
    "\n",
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_input):\n",
    "\t# fit model\n",
    "\tmodel = build_model(train, n_input)\n",
    "\t# history is a list of weekly data\n",
    "\thistory = [x for x in train]\n",
    "\t# walk-forward validation over each week\n",
    "\tpredictions = list()\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# predict the week\n",
    "\t\tyhat_sequence = forecast(model, history, n_input)\n",
    "\t\t# store the predictions\n",
    "\t\tpredictions.append(yhat_sequence)\n",
    "\t\t# get real observation and add to history for predicting the next week\n",
    "\t\thistory.append(test[i, :])\n",
    "\t# evaluate predictions days for each week\n",
    "\tpredictions = array(predictions)\n",
    "\tscore, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "\treturn score, scores\n",
    "\n",
    "# load the new file\n",
    "dataset = read_csv('../data/household_power_consumption_days.csv', header=0, infer_datetime_format=True, parse_dates=['datetime'], index_col=['datetime'])\n",
    "# split into train and test\n",
    "train, test = split_dataset(dataset.values)\n",
    "# evaluate model and get scores\n",
    "n_input = 14\n",
    "score, scores = evaluate_model(train, test, n_input)\n",
    "# summarize scores\n",
    "summarize_scores('lstm', score, scores)\n",
    "# plot scores\n",
    "days = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\n",
    "pyplot.plot(days, scores, marker='o', label='lstm')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multichannel multi-step cnn\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "\t# split into standard weeks\n",
    "\ttrain, test = data[1:-328], data[-328:-6]\n",
    "\t# restructure into windows of weekly data\n",
    "\ttrain = array(split(train, len(train)/7))\n",
    "\ttest = array(split(test, len(test)/7))\n",
    "\treturn train, test\n",
    "\n",
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "\tscores = list()\n",
    "\t# calculate an RMSE score for each day\n",
    "\tfor i in range(actual.shape[1]):\n",
    "\t\t# calculate mse\n",
    "\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "\t\t# calculate rmse\n",
    "\t\trmse = sqrt(mse)\n",
    "\t\t# store\n",
    "\t\tscores.append(rmse)\n",
    "\t# calculate overall RMSE\n",
    "\ts = 0\n",
    "\tfor row in range(actual.shape[0]):\n",
    "\t\tfor col in range(actual.shape[1]):\n",
    "\t\t\ts += (actual[row, col] - predicted[row, col])**2\n",
    "\tscore = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "\treturn score, scores\n",
    "\n",
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "\ts_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "\tprint('%s: [%.3f] %s' % (name, score, s_scores))\n",
    "\n",
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=7):\n",
    "\t# flatten data\n",
    "\tdata = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "\tX, y = list(), list()\n",
    "\tin_start = 0\n",
    "\t# step over the entire history one time step at a time\n",
    "\tfor _ in range(len(data)):\n",
    "\t\t# define the end of the input sequence\n",
    "\t\tin_end = in_start + n_input\n",
    "\t\tout_end = in_end + n_out\n",
    "\t\t# ensure we have enough data for this instance\n",
    "\t\tif out_end < len(data):\n",
    "\t\t\tX.append(data[in_start:in_end, :])\n",
    "\t\t\ty.append(data[in_end:out_end, 0])\n",
    "\t\t# move along one time step\n",
    "\t\tin_start += 1\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# train the model\n",
    "def build_model(train, n_input):\n",
    "\t# prepare data\n",
    "\ttrain_x, train_y = to_supervised(train, n_input)\n",
    "\t# define parameters\n",
    "\tverbose, epochs, batch_size = 1, 70, 16\n",
    "\tn_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "\tmodel.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "\tmodel.add(MaxPooling1D(pool_size=2))\n",
    "\tmodel.add(Conv1D(filters=16, kernel_size=3, activation='relu'))\n",
    "\tmodel.add(MaxPooling1D(pool_size=2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(100, activation='relu'))\n",
    "\tmodel.add(Dense(n_outputs))\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\t# fit network\n",
    "\tmodel.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\treturn model\n",
    "\n",
    "# make a forecast\n",
    "def forecast(model, history, n_input):\n",
    "\t# flatten data\n",
    "\tdata = array(history)\n",
    "\tdata = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "\t# retrieve last observations for input data\n",
    "\tinput_x = data[-n_input:, :]\n",
    "\t# reshape into [1, n_input, n]\n",
    "\tinput_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
    "\t# forecast the next week\n",
    "\tyhat = model.predict(input_x, verbose=1)\n",
    "\t# we only want the vector forecast\n",
    "\tyhat = yhat[0]\n",
    "\treturn yhat\n",
    "\n",
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_input):\n",
    "\t# fit model\n",
    "\tmodel = build_model(train, n_input)\n",
    "\t# history is a list of weekly data\n",
    "\thistory = [x for x in train]\n",
    "\t# walk-forward validation over each week\n",
    "\tpredictions = list()\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# predict the week\n",
    "\t\tyhat_sequence = forecast(model, history, n_input)\n",
    "\t\t# store the predictions\n",
    "\t\tpredictions.append(yhat_sequence)\n",
    "\t\t# get real observation and add to history for predicting the next week\n",
    "\t\thistory.append(test[i, :])\n",
    "\t# evaluate predictions days for each week\n",
    "\tpredictions = array(predictions)\n",
    "\tscore, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "\treturn score, scores\n",
    "\n",
    "# load the new file\n",
    "dataset = read_csv('../data/selected_data_ISONE1.csv', header=0, infer_datetime_format=True, parse_dates=['datetime'], index_col=['datetime'])\n",
    "# split into train and test\n",
    "train, test = split_dataset(dataset.values)\n",
    "# evaluate model and get scores\n",
    "n_input = 14\n",
    "score, scores = evaluate_model(train, test, n_input)\n",
    "# summarize scores\n",
    "summarize_scores('cnn', score, scores)\n",
    "# plot scores\n",
    "days = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\n",
    "pyplot.plot(days, scores, marker='o', label='cnn')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_csv('../data/selected_data_ISONE1.csv', header=0, infer_datetime_format=True, parse_dates=['date'], index_col=['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split into standard weeks\n",
    "#2013/12/31\n",
    "train = d[0:95016]\n",
    "#2014/1/1\n",
    "test = d[95016:103776]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# multichannel multi-step cnn\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "\t# split into standard weeks\n",
    "\ttrain, test = data[0:95016], data[95016:103776]\n",
    "\t# restructure into windows of 6hours data\n",
    "\ttrain = array(split(train, len(train)/24))\n",
    "\ttest = array(split(test, len(test)/24))\n",
    "\treturn train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.2863e+04 2.3000e+01]\n",
      "  [1.2389e+04 2.2000e+01]\n",
      "  [1.2155e+04 2.1000e+01]\n",
      "  ...\n",
      "  [1.4753e+04 3.4000e+01]\n",
      "  [1.3757e+04 3.3000e+01]\n",
      "  [1.2707e+04 3.3000e+01]]\n",
      "\n",
      " [[1.1861e+04 3.2000e+01]\n",
      "  [1.1373e+04 3.2000e+01]\n",
      "  [1.1085e+04 3.2000e+01]\n",
      "  ...\n",
      "  [1.4745e+04 3.6000e+01]\n",
      "  [1.3444e+04 3.6000e+01]\n",
      "  [1.2350e+04 3.5000e+01]]\n",
      "\n",
      " [[1.1657e+04 3.5000e+01]\n",
      "  [1.1358e+04 3.4000e+01]\n",
      "  [1.1315e+04 3.1000e+01]\n",
      "  ...\n",
      "  [1.7930e+04 9.0000e+00]\n",
      "  [1.6416e+04 8.0000e+00]\n",
      "  [1.5059e+04 7.0000e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1.2102e+04 3.2000e+01]\n",
      "  [1.1599e+04 3.2000e+01]\n",
      "  [1.1287e+04 3.2000e+01]\n",
      "  ...\n",
      "  [1.4896e+04 3.7000e+01]\n",
      "  [1.3727e+04 3.8000e+01]\n",
      "  [1.2579e+04 3.7000e+01]]\n",
      "\n",
      " [[1.1726e+04 3.6000e+01]\n",
      "  [1.1262e+04 3.5000e+01]\n",
      "  [1.1084e+04 3.4000e+01]\n",
      "  ...\n",
      "  [1.6758e+04 1.9000e+01]\n",
      "  [1.5526e+04 1.7000e+01]\n",
      "  [1.4313e+04 1.6000e+01]]\n",
      "\n",
      " [[1.3429e+04 1.5000e+01]\n",
      "  [1.3009e+04 1.3000e+01]\n",
      "  [1.2795e+04 1.3000e+01]\n",
      "  ...\n",
      "  [1.6172e+04 2.0000e+01]\n",
      "  [1.5355e+04 2.0000e+01]\n",
      "  [1.4605e+04 2.0000e+01]]]\n"
     ]
    }
   ],
   "source": [
    "dataset = read_csv('../data/selected_data_ISONE1.csv', header=0, infer_datetime_format=True, parse_dates=['date'], index_col=['date'])\n",
    "data = dataset.values\n",
    "# split into train and test\n",
    "train, test = split_dataset(data)\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.3821e+04  2.0000e+01]\n",
      "  [ 1.3280e+04  1.9000e+01]\n",
      "  [ 1.2885e+04  1.7000e+01]\n",
      "  ...\n",
      "  [ 1.6159e+04  2.1000e+01]\n",
      "  [ 1.4912e+04  2.0000e+01]\n",
      "  [ 1.3783e+04  2.0000e+01]]\n",
      "\n",
      " [[ 1.3041e+04  1.9000e+01]\n",
      "  [ 1.2682e+04  1.8000e+01]\n",
      "  [ 1.2517e+04  1.7000e+01]\n",
      "  ...\n",
      "  [ 1.8057e+04  5.0000e+00]\n",
      "  [ 1.6807e+04  4.0000e+00]\n",
      "  [ 1.5683e+04  3.0000e+00]]\n",
      "\n",
      " [[ 1.4922e+04  2.0000e+00]\n",
      "  [ 1.4443e+04  2.0000e+00]\n",
      "  [ 1.4268e+04  2.0000e+00]\n",
      "  ...\n",
      "  [ 1.8600e+04  1.0000e+00]\n",
      "  [ 1.7444e+04  0.0000e+00]\n",
      "  [ 1.6330e+04 -1.0000e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.1126e+04  3.7000e+01]\n",
      "  [ 1.0701e+04  3.6000e+01]\n",
      "  [ 1.0552e+04  3.5000e+01]\n",
      "  ...\n",
      "  [ 1.5567e+04  3.0000e+01]\n",
      "  [ 1.4305e+04  2.9000e+01]\n",
      "  [ 1.3110e+04  2.8000e+01]]\n",
      "\n",
      " [[ 1.2286e+04  2.7000e+01]\n",
      "  [ 1.1848e+04  2.6000e+01]\n",
      "  [ 1.1683e+04  2.4000e+01]\n",
      "  ...\n",
      "  [ 1.6458e+04  2.0000e+01]\n",
      "  [ 1.5188e+04  1.8000e+01]\n",
      "  [ 1.3992e+04  1.7000e+01]]\n",
      "\n",
      " [[ 1.3152e+04  1.7000e+01]\n",
      "  [ 1.2666e+04  1.6000e+01]\n",
      "  [ 1.2446e+04  1.6000e+01]\n",
      "  ...\n",
      "  [ 1.5525e+04  2.1000e+01]\n",
      "  [ 1.4759e+04  1.8000e+01]\n",
      "  [ 1.4071e+04  1.9000e+01]]]\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "def build_model(train, n_input):\n",
    "\t# prepare data\n",
    "\ttrain_x, train_y = to_supervised(train, n_input)\n",
    "\t# define parameters\n",
    "\t#verbose, epochs, batch_size = 1, 6, 16\n",
    "\tn_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "\tmodel.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "\tmodel.add(MaxPooling1D(pool_size=2))\n",
    "\tmodel.add(Conv1D(filters=16, kernel_size=3, activation='relu'))\n",
    "\tmodel.add(MaxPooling1D(pool_size=2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(100, activation='relu'))\n",
    "\tmodel.add(Dense(n_outputs))\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\t# fit network\n",
    "\t#model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=24):\n",
    "\t# flatten data\n",
    "\tdata = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "\tX, y = list(), list()\n",
    "\tin_start = 0\n",
    "\t# step over the entire history one time step at a time\n",
    "\tfor _ in range(len(data)):\n",
    "\t\t# define the end of the input sequence\n",
    "\t\tin_end = in_start + n_input\n",
    "\t\tout_end = in_end + n_out\n",
    "\t\t# ensure we have enough data for this instance\n",
    "\t\tif out_end < len(data):\n",
    "\t\t\tX.append(data[in_start:in_end, :])\n",
    "\t\t\ty.append(data[in_end:out_end, 0])\n",
    "\t\t# move along one time step\n",
    "\t\tin_start += 1\n",
    "\treturn array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_input,model):\n",
    "\t# fit model\n",
    "\t#model = modeltest\n",
    "\t# history is a list of weekly data\n",
    "\thistory = [x for x in train]\n",
    "\t# walk-forward validation over each week\n",
    "\tpredictions = list()\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# predict the week\n",
    "\t\tyhat_sequence = forecast(model, history, n_input)\n",
    "\t\t# store the predictions\n",
    "\t\tpredictions.append(yhat_sequence)\n",
    "\t\t# get real observation and add to history for predicting the next week\n",
    "\t\thistory.append(test[i, :])\n",
    "\t# evaluate predictions days for each week\n",
    "\tpredictions = array(predictions)\n",
    "\tscore, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "\treturn score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a forecast\n",
    "def forecast(model, history, n_input):\n",
    "\t# flatten data\n",
    "\tdata = array(history)\n",
    "\tdata = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "\t# retrieve last observations for input data\n",
    "\tinput_x = data[-n_input:, :]\n",
    "\t# reshape into [1, n_input, n]\n",
    "\tinput_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
    "\t# forecast the next week\n",
    "\tyhat = model.predict(input_x, verbose=1)\n",
    "\t# we only want the vector forecast\n",
    "\tyhat = yhat[0]\n",
    "\treturn yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "\tscores = list()\n",
    "\t# calculate an RMSE score for each day\n",
    "\tfor i in range(actual.shape[1]):\n",
    "\t\t# calculate mse\n",
    "\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "\t\t# calculate rmse\n",
    "\t\trmse = sqrt(mse)\n",
    "\t\t# store\n",
    "\t\tscores.append(rmse)\n",
    "\t# calculate overall RMSE\n",
    "\ts = 0\n",
    "\tfor row in range(actual.shape[0]):\n",
    "\t\tfor col in range(actual.shape[1]):\n",
    "\t\t\ts += (actual[row, col] - predicted[row, col])**2\n",
    "\tscore = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "\treturn score, scores\n",
    "\n",
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "\ts_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "\tprint('%s: [%.3f] %s' % (name, score, s_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = to_supervised(train, n_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.2863e+04 2.3000e+01]\n",
      "  [1.2389e+04 2.2000e+01]\n",
      "  [1.2155e+04 2.1000e+01]\n",
      "  ...\n",
      "  [1.4753e+04 3.4000e+01]\n",
      "  [1.3757e+04 3.3000e+01]\n",
      "  [1.2707e+04 3.3000e+01]]\n",
      "\n",
      " [[1.2389e+04 2.2000e+01]\n",
      "  [1.2155e+04 2.1000e+01]\n",
      "  [1.2072e+04 2.1000e+01]\n",
      "  ...\n",
      "  [1.3757e+04 3.3000e+01]\n",
      "  [1.2707e+04 3.3000e+01]\n",
      "  [1.1861e+04 3.2000e+01]]\n",
      "\n",
      " [[1.2155e+04 2.1000e+01]\n",
      "  [1.2072e+04 2.1000e+01]\n",
      "  [1.2160e+04 2.2000e+01]\n",
      "  ...\n",
      "  [1.2707e+04 3.3000e+01]\n",
      "  [1.1861e+04 3.2000e+01]\n",
      "  [1.1373e+04 3.2000e+01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1.4896e+04 3.7000e+01]\n",
      "  [1.3727e+04 3.8000e+01]\n",
      "  [1.2579e+04 3.7000e+01]\n",
      "  ...\n",
      "  [1.8649e+04 2.4000e+01]\n",
      "  [1.8268e+04 2.2000e+01]\n",
      "  [1.7727e+04 2.1000e+01]]\n",
      "\n",
      " [[1.3727e+04 3.8000e+01]\n",
      "  [1.2579e+04 3.7000e+01]\n",
      "  [1.1726e+04 3.6000e+01]\n",
      "  ...\n",
      "  [1.8268e+04 2.2000e+01]\n",
      "  [1.7727e+04 2.1000e+01]\n",
      "  [1.6758e+04 1.9000e+01]]\n",
      "\n",
      " [[1.2579e+04 3.7000e+01]\n",
      "  [1.1726e+04 3.6000e+01]\n",
      "  [1.1262e+04 3.5000e+01]\n",
      "  ...\n",
      "  [1.7727e+04 2.1000e+01]\n",
      "  [1.6758e+04 1.9000e+01]\n",
      "  [1.5526e+04 1.7000e+01]]]\n"
     ]
    }
   ],
   "source": [
    "print(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11861., 11373., 11085., ..., 14745., 13444., 12350.],\n",
       "       [11373., 11085., 10905., ..., 13444., 12350., 11657.],\n",
       "       [11085., 10905., 10930., ..., 12350., 11657., 11358.],\n",
       "       ...,\n",
       "       [16758., 15526., 14313., ..., 18538., 17711., 16938.],\n",
       "       [15526., 14313., 13429., ..., 17711., 16938., 16172.],\n",
       "       [14313., 13429., 13009., ..., 16938., 16172., 15355.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "94968/94968 [==============================] - 29s 308us/step - loss: 2526929.1972\n",
      "Epoch 2/6\n",
      "94968/94968 [==============================] - 29s 303us/step - loss: 1300100.8311\n",
      "Epoch 3/6\n",
      "94968/94968 [==============================] - 29s 304us/step - loss: 1139300.1546\n",
      "Epoch 4/6\n",
      "94968/94968 [==============================] - 29s 307us/step - loss: 1054938.1365\n",
      "Epoch 5/6\n",
      "94968/94968 [==============================] - 29s 305us/step - loss: 991778.1702\n",
      "Epoch 6/6\n",
      "94968/94968 [==============================] - 29s 304us/step - loss: 949179.8512\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "import pickle\n",
    "all_mae_histories = []\n",
    "verbose, epochs, batch_size = 1, 6, 16\n",
    "model = build_model(train, n_input)\n",
    "history = model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "mae_history = history.history['loss']\n",
    "all_mae_histories.append(mae_history)\n",
    "\n",
    "\n",
    "file = open('history76.pkl', 'wb')\n",
    "pickle.dump(all_mae_histories, file)\n",
    "file.close()\n",
    "\n",
    "model.save('model76.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model63.h5')\n",
    "\n",
    "file = open('history63.pkl', 'rb')\n",
    "\n",
    "a = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2526929.197166414,\n",
       "  1300100.831132592,\n",
       "  1139300.1546415635,\n",
       "  1054938.1364775503,\n",
       "  991778.1702152304,\n",
       "  949179.8511761856]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mae_histories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "average_mae_history = [\n",
    "    np.mean([x[i] for x in all_mae_histories]) for i in range(epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 348ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 997us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 999us/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 996us/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 999us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 996us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 995us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 996us/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "cnn: [901.067] 463.7, 782.2, 491.1, 527.9, 492.6, 560.6, 895.2, 1040.3, 884.4, 831.1, 861.7, 885.6, 972.2, 1063.8, 1102.7, 1120.7, 1117.2, 1132.6, 1073.5, 1010.0, 1050.4, 1006.3, 863.1, 765.6\n"
     ]
    }
   ],
   "source": [
    "score, scores = evaluate_model(train, test, n_input,model)\n",
    "# summarize scores\n",
    "summarize_scores('cnn', score, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

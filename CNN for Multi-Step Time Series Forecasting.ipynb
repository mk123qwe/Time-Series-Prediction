{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multichannel multi-step cnn\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "\t# split into standard weeks\n",
    "\ttrain, test = data[0:95016], data[95016:103776]\n",
    "\t# restructure into windows of 6hours data\n",
    "\ttrain = array(split(train, len(train)/24))\n",
    "\ttest = array(split(test, len(test)/24))\n",
    "\treturn train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_csv('../data/selected_data_ISONE1.csv', header=0, infer_datetime_format=True, parse_dates=['date'], index_col=['date'])\n",
    "data = dataset.values\n",
    "# split into train and test\n",
    "train, test = split_dataset(data)\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "def build_model(train, n_input):\n",
    "\t# prepare data\n",
    "\ttrain_x, train_y = to_supervised(train, n_input)\n",
    "\t# define parameters\n",
    "\t#verbose, epochs, batch_size = 1, 6, 16\n",
    "\tn_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "\tmodel.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "\tmodel.add(MaxPooling1D(pool_size=2))\n",
    "\tmodel.add(Conv1D(filters=16, kernel_size=3, activation='relu'))\n",
    "\tmodel.add(MaxPooling1D(pool_size=2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(100, activation='relu'))\n",
    "\tmodel.add(Dense(n_outputs))\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\t# fit network\n",
    "\t#model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=24):\n",
    "\t# flatten data\n",
    "\tdata = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "\tX, y = list(), list()\n",
    "\tin_start = 0\n",
    "\t# step over the entire history one time step at a time\n",
    "\tfor _ in range(len(data)):\n",
    "\t\t# define the end of the input sequence\n",
    "\t\tin_end = in_start + n_input\n",
    "\t\tout_end = in_end + n_out\n",
    "\t\t# ensure we have enough data for this instance\n",
    "\t\tif out_end < len(data):\n",
    "\t\t\tX.append(data[in_start:in_end, :])\n",
    "\t\t\ty.append(data[in_end:out_end, 0])\n",
    "\t\t# move along one time step\n",
    "\t\tin_start += 1\n",
    "\treturn array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_input,model):\n",
    "\t# fit model\n",
    "\t#model = modeltest\n",
    "\t# history is a list of weekly data\n",
    "\thistory = [x for x in train]\n",
    "\t# walk-forward validation over each week\n",
    "\tpredictions = list()\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# predict the week\n",
    "\t\tyhat_sequence = forecast(model, history, n_input)\n",
    "\t\t# store the predictions\n",
    "\t\tpredictions.append(yhat_sequence)\n",
    "\t\t# get real observation and add to history for predicting the next week\n",
    "\t\thistory.append(test[i, :])\n",
    "\t# evaluate predictions days for each week\n",
    "\tpredictions = array(predictions)\n",
    "\tscore, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "\treturn score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a forecast\n",
    "def forecast(model, history, n_input):\n",
    "\t# flatten data\n",
    "\tdata = array(history)\n",
    "\tdata = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "\t# retrieve last observations for input data\n",
    "\tinput_x = data[-n_input:, :]\n",
    "\t# reshape into [1, n_input, n]\n",
    "\tinput_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
    "\t# forecast the next week\n",
    "\tyhat = model.predict(input_x, verbose=1)\n",
    "\t# we only want the vector forecast\n",
    "\tyhat = yhat[0]\n",
    "\treturn yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "\tscores = list()\n",
    "\t# calculate an RMSE score for each day\n",
    "\tfor i in range(actual.shape[1]):\n",
    "\t\t# calculate mse\n",
    "\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "\t\t# calculate rmse\n",
    "\t\trmse = sqrt(mse)\n",
    "\t\t# store\n",
    "\t\tscores.append(rmse)\n",
    "\t# calculate overall RMSE\n",
    "\ts = 0\n",
    "\tfor row in range(actual.shape[0]):\n",
    "\t\tfor col in range(actual.shape[1]):\n",
    "\t\t\ts += (actual[row, col] - predicted[row, col])**2\n",
    "\tscore = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "\treturn score, scores\n",
    "\n",
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "\ts_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "\tprint('%s: [%.3f] %s' % (name, score, s_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "import pickle\n",
    "all_mae_histories = []\n",
    "verbose, epochs, batch_size = 1, 6, 16\n",
    "model = build_model(train, n_input)\n",
    "history = model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "mae_history = history.history['loss']\n",
    "all_mae_histories.append(mae_history)\n",
    "\n",
    "\n",
    "file = open('history76.pkl', 'wb')\n",
    "pickle.dump(all_mae_histories, file)\n",
    "file.close()\n",
    "\n",
    "model.save('model76.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model63.h5')\n",
    "\n",
    "file = open('history63.pkl', 'rb')\n",
    "\n",
    "a = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mae_histories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "average_mae_history = [\n",
    "    np.mean([x[i] for x in all_mae_histories]) for i in range(epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, scores = evaluate_model(train, test, n_input,model)\n",
    "# summarize scores\n",
    "summarize_scores('cnn', score, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi headed multi-step cnn\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "\t# split into standard weeks\n",
    "\ttrain, test = data[1:-328], data[-328:-6]\n",
    "\t# restructure into windows of weekly data\n",
    "\ttrain = array(split(train, len(train)/7))\n",
    "\ttest = array(split(test, len(test)/7))\n",
    "\treturn train, test\n",
    "\n",
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "\tscores = list()\n",
    "\t# calculate an RMSE score for each day\n",
    "\tfor i in range(actual.shape[1]):\n",
    "\t\t# calculate mse\n",
    "\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "\t\t# calculate rmse\n",
    "\t\trmse = sqrt(mse)\n",
    "\t\t# store\n",
    "\t\tscores.append(rmse)\n",
    "\t# calculate overall RMSE\n",
    "\ts = 0\n",
    "\tfor row in range(actual.shape[0]):\n",
    "\t\tfor col in range(actual.shape[1]):\n",
    "\t\t\ts += (actual[row, col] - predicted[row, col])**2\n",
    "\tscore = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "\treturn score, scores\n",
    "\n",
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "\ts_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "\tprint('%s: [%.3f] %s' % (name, score, s_scores))\n",
    "\n",
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=7):\n",
    "\t# flatten data\n",
    "\tdata = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "\tX, y = list(), list()\n",
    "\tin_start = 0\n",
    "\t# step over the entire history one time step at a time\n",
    "\tfor _ in range(len(data)):\n",
    "\t\t# define the end of the input sequence\n",
    "\t\tin_end = in_start + n_input\n",
    "\t\tout_end = in_end + n_out\n",
    "\t\t# ensure we have enough data for this instance\n",
    "\t\tif out_end < len(data):\n",
    "\t\t\tX.append(data[in_start:in_end, :])\n",
    "\t\t\ty.append(data[in_end:out_end, 0])\n",
    "\t\t# move along one time step\n",
    "\t\tin_start += 1\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# plot training history\n",
    "def plot_history(history):\n",
    "\t# plot loss\n",
    "\tpyplot.subplot(2, 1, 1)\n",
    "\tpyplot.plot(history.history['loss'], label='train')\n",
    "\tpyplot.plot(history.history['val_loss'], label='test')\n",
    "\tpyplot.title('loss', y=0, loc='center')\n",
    "\tpyplot.legend()\n",
    "\t# plot rmse\n",
    "\tpyplot.subplot(2, 1, 2)\n",
    "\tpyplot.plot(history.history['rmse'], label='train')\n",
    "\tpyplot.plot(history.history['val_rmse'], label='test')\n",
    "\tpyplot.title('rmse', y=0, loc='center')\n",
    "\tpyplot.legend()\n",
    "\tpyplot.show()\n",
    "\n",
    "# train the model\n",
    "def build_model(train, n_input):\n",
    "\t# prepare data\n",
    "\ttrain_x, train_y = to_supervised(train, n_input)\n",
    "\t# define parameters\n",
    "\tverbose, epochs, batch_size = 0, 25, 16\n",
    "\tn_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "\t# create a channel for each variable\n",
    "\tin_layers, out_layers = list(), list()\n",
    "\tfor i in range(n_features):\n",
    "\t\tinputs = Input(shape=(n_timesteps,1))\n",
    "\t\tconv1 = Conv1D(filters=32, kernel_size=3, activation='relu')(inputs)\n",
    "\t\tconv2 = Conv1D(filters=32, kernel_size=3, activation='relu')(conv1)\n",
    "\t\tpool1 = MaxPooling1D(pool_size=2)(conv2)\n",
    "\t\tflat = Flatten()(pool1)\n",
    "\t\t# store layers\n",
    "\t\tin_layers.append(inputs)\n",
    "\t\tout_layers.append(flat)\n",
    "\t# merge heads\n",
    "\tmerged = concatenate(out_layers)\n",
    "\t# interpretation\n",
    "\tdense1 = Dense(200, activation='relu')(merged)\n",
    "\tdense2 = Dense(100, activation='relu')(dense1)\n",
    "\toutputs = Dense(n_outputs)(dense2)\n",
    "\tmodel = Model(inputs=in_layers, outputs=outputs)\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\t# fit network\n",
    "\tinput_data = [train_x[:,:,i].reshape((train_x.shape[0],n_timesteps,1)) for i in range(n_features)]\n",
    "\tmodel.fit(input_data, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\treturn model\n",
    "\n",
    "# make a forecast\n",
    "def forecast(model, history, n_input):\n",
    "\t# flatten data\n",
    "\tdata = array(history)\n",
    "\tdata = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "\t# retrieve last observations for input data\n",
    "\tinput_x = data[-n_input:, :]\n",
    "\t# reshape into n input arrays\n",
    "\tinput_x = [input_x[:,i].reshape((1,input_x.shape[0],1)) for i in range(input_x.shape[1])]\n",
    "\t# forecast the next week\n",
    "\tyhat = model.predict(input_x, verbose=0)\n",
    "\t# we only want the vector forecast\n",
    "\tyhat = yhat[0]\n",
    "\treturn yhat\n",
    "\n",
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_input):\n",
    "\t# fit model\n",
    "\tmodel = build_model(train, n_input)\n",
    "\t# history is a list of weekly data\n",
    "\thistory = [x for x in train]\n",
    "\t# walk-forward validation over each week\n",
    "\tpredictions = list()\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# predict the week\n",
    "\t\tyhat_sequence = forecast(model, history, n_input)\n",
    "\t\t# store the predictions\n",
    "\t\tpredictions.append(yhat_sequence)\n",
    "\t\t# get real observation and add to history for predicting the next week\n",
    "\t\thistory.append(test[i, :])\n",
    "\t# evaluate predictions days for each week\n",
    "\tpredictions = array(predictions)\n",
    "\tscore, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "\treturn score, scores\n",
    "\n",
    "# load the new file\n",
    "dataset = read_csv('household_power_consumption_days.csv', header=0, infer_datetime_format=True, parse_dates=['datetime'], index_col=['datetime'])\n",
    "# split into train and test\n",
    "train, test = split_dataset(dataset.values)\n",
    "# evaluate model and get scores\n",
    "n_input = 14\n",
    "score, scores = evaluate_model(train, test, n_input)\n",
    "# summarize scores\n",
    "summarize_scores('cnn', score, scores)\n",
    "# plot scores\n",
    "days = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\n",
    "pyplot.plot(days, scores, marker='o', label='cnn')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# multi headed multi-step cnn\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "\t# split into standard weeks\n",
    "\ttrain, test = data[0:95016], data[95016:103776]\n",
    "\t# restructure into windows of 6hours data\n",
    "\ttrain = array(split(train, len(train)/24))\n",
    "\ttest = array(split(test, len(test)/24))\n",
    "\treturn train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "\tscores = list()\n",
    "\t# calculate an RMSE score for each day\n",
    "\tfor i in range(actual.shape[1]):\n",
    "\t\t# calculate mse\n",
    "\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "\t\t# calculate rmse\n",
    "\t\trmse = sqrt(mse)\n",
    "\t\t# store\n",
    "\t\tscores.append(rmse)\n",
    "\t# calculate overall RMSE\n",
    "\ts = 0\n",
    "\tfor row in range(actual.shape[0]):\n",
    "\t\tfor col in range(actual.shape[1]):\n",
    "\t\t\ts += (actual[row, col] - predicted[row, col])**2\n",
    "\tscore = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "\treturn score, scores\n",
    "\n",
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "\ts_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "\tprint('%s: [%.3f] %s' % (name, score, s_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=24):\n",
    "\t# flatten data\n",
    "\tdata = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "\tX, y = list(), list()\n",
    "\tin_start = 0\n",
    "\t# step over the entire history one time step at a time\n",
    "\tfor _ in range(len(data)):\n",
    "\t\t# define the end of the input sequence\n",
    "\t\tin_end = in_start + n_input\n",
    "\t\tout_end = in_end + n_out\n",
    "\t\t# ensure we have enough data for this instance\n",
    "\t\tif out_end < len(data):\n",
    "\t\t\tX.append(data[in_start:in_end, :])\n",
    "\t\t\ty.append(data[in_end:out_end, 0])\n",
    "\t\t# move along one time step\n",
    "\t\tin_start += 1\n",
    "\treturn array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "def build_model(train, n_input):\n",
    "\t# prepare data\n",
    "\ttrain_x, train_y = to_supervised(train, n_input)\n",
    "\t# define parameters\n",
    "\tverbose, epochs, batch_size = 1, 6, 16\n",
    "\tn_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "\t# create a channel for each variable\n",
    "\tin_layers, out_layers = list(), list()\n",
    "\tfor i in range(n_features):\n",
    "\t\tinputs = Input(shape=(n_timesteps,1))\n",
    "\t\tconv1 = Conv1D(filters=32, kernel_size=3, activation='relu')(inputs)\n",
    "\t\tconv2 = Conv1D(filters=32, kernel_size=3, activation='relu')(conv1)\n",
    "\t\tpool1 = MaxPooling1D(pool_size=2)(conv2)\n",
    "\t\tflat = Flatten()(pool1)\n",
    "\t\t# store layers\n",
    "\t\tin_layers.append(inputs)\n",
    "\t\tout_layers.append(flat)\n",
    "\t# merge heads\n",
    "\tmerged = concatenate(out_layers)\n",
    "\t# interpretation\n",
    "\tdense1 = Dense(200, activation='relu')(merged)\n",
    "\tdense2 = Dense(100, activation='relu')(dense1)\n",
    "\toutputs = Dense(n_outputs)(dense2)\n",
    "\tmodel = Model(inputs=in_layers, outputs=outputs)\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\t# fit network\n",
    "\tinput_data = [train_x[:,:,i].reshape((train_x.shape[0],n_timesteps,1)) for i in range(n_features)]\n",
    "\t#model.fit(input_data, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\treturn model,input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a forecast\n",
    "def forecast(model, history, n_input):\n",
    "\t# flatten data\n",
    "\tdata = array(history)\n",
    "\tdata = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "\t# retrieve last observations for input data\n",
    "\tinput_x = data[-n_input:, :]\n",
    "\t# reshape into n input arrays\n",
    "\tinput_x = [input_x[:,i].reshape((1,input_x.shape[0],1)) for i in range(input_x.shape[1])]\n",
    "\t# forecast the next week\n",
    "\tyhat = model.predict(input_x, verbose=1)\n",
    "\t# we only want the vector forecast\n",
    "\tyhat = yhat[0]\n",
    "\treturn yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "#def evaluate_model(train, test, n_input):\n",
    "def evaluate_model(train, test, n_input,model):\n",
    "\t# fit model\n",
    "\t#model = build_model(train, n_input)\n",
    "\t# history is a list of weekly data\n",
    "\thistory = [x for x in train]\n",
    "\t# walk-forward validation over each week\n",
    "\tpredictions = list()\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# predict the week\n",
    "\t\tyhat_sequence = forecast(model, history, n_input)\n",
    "\t\t# store the predictions\n",
    "\t\tpredictions.append(yhat_sequence)\n",
    "\t\t# get real observation and add to history for predicting the next week\n",
    "\t\thistory.append(test[i, :])\n",
    "\t# evaluate predictions days for each week\n",
    "\tpredictions = array(predictions)\n",
    "\tscore, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "\treturn score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = read_csv('../data/selected_data_ISONE1.csv', header=0, infer_datetime_format=True, parse_dates=['date'], index_col=['date'])\n",
    "data = dataset.values\n",
    "# split into train and test\n",
    "train, test = split_dataset(data)\n",
    "# evaluate model and get scores\n",
    "n_input = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "94968/94968 [==============================] - 37s 387us/step - loss: 1912675.9713\n",
      "Epoch 2/6\n",
      "94968/94968 [==============================] - ETA: 0s - loss: 1115793.845 - 36s 383us/step - loss: 1115856.9652\n",
      "Epoch 3/6\n",
      "94968/94968 [==============================] - 35s 372us/step - loss: 964160.8077\n",
      "Epoch 4/6\n",
      "94968/94968 [==============================] - 37s 389us/step - loss: 885062.6933\n",
      "Epoch 5/6\n",
      "94968/94968 [==============================] - 36s 375us/step - loss: 814677.2543\n",
      "Epoch 6/6\n",
      "94968/94968 [==============================] - 36s 377us/step - loss: 781116.8768\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "import pickle\n",
    "all_mae_histories = []\n",
    "verbose, epochs, batch_size = 1, 6, 16\n",
    "train_x, train_y = to_supervised(train, n_input)\n",
    "model,input_data = build_model(train, n_input)\n",
    "history = model.fit(input_data, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "mae_history = history.history['loss']\n",
    "all_mae_histories.append(mae_history)\n",
    "\n",
    "\n",
    "file = open('history77.pkl', 'wb')\n",
    "pickle.dump(all_mae_histories, file)\n",
    "file.close()\n",
    "\n",
    "model.save('model77.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[12863.],\n",
       "         [12389.],\n",
       "         [12155.],\n",
       "         ...,\n",
       "         [14753.],\n",
       "         [13757.],\n",
       "         [12707.]],\n",
       " \n",
       "        [[12389.],\n",
       "         [12155.],\n",
       "         [12072.],\n",
       "         ...,\n",
       "         [13757.],\n",
       "         [12707.],\n",
       "         [11861.]],\n",
       " \n",
       "        [[12155.],\n",
       "         [12072.],\n",
       "         [12160.],\n",
       "         ...,\n",
       "         [12707.],\n",
       "         [11861.],\n",
       "         [11373.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[14896.],\n",
       "         [13727.],\n",
       "         [12579.],\n",
       "         ...,\n",
       "         [18649.],\n",
       "         [18268.],\n",
       "         [17727.]],\n",
       " \n",
       "        [[13727.],\n",
       "         [12579.],\n",
       "         [11726.],\n",
       "         ...,\n",
       "         [18268.],\n",
       "         [17727.],\n",
       "         [16758.]],\n",
       " \n",
       "        [[12579.],\n",
       "         [11726.],\n",
       "         [11262.],\n",
       "         ...,\n",
       "         [17727.],\n",
       "         [16758.],\n",
       "         [15526.]]]), array([[[23.],\n",
       "         [22.],\n",
       "         [21.],\n",
       "         ...,\n",
       "         [34.],\n",
       "         [33.],\n",
       "         [33.]],\n",
       " \n",
       "        [[22.],\n",
       "         [21.],\n",
       "         [21.],\n",
       "         ...,\n",
       "         [33.],\n",
       "         [33.],\n",
       "         [32.]],\n",
       " \n",
       "        [[21.],\n",
       "         [21.],\n",
       "         [22.],\n",
       "         ...,\n",
       "         [33.],\n",
       "         [32.],\n",
       "         [32.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[37.],\n",
       "         [38.],\n",
       "         [37.],\n",
       "         ...,\n",
       "         [24.],\n",
       "         [22.],\n",
       "         [21.]],\n",
       " \n",
       "        [[38.],\n",
       "         [37.],\n",
       "         [36.],\n",
       "         ...,\n",
       "         [22.],\n",
       "         [21.],\n",
       "         [19.]],\n",
       " \n",
       "        [[37.],\n",
       "         [36.],\n",
       "         [35.],\n",
       "         ...,\n",
       "         [21.],\n",
       "         [19.],\n",
       "         [17.]]])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11861., 11373., 11085., ..., 14745., 13444., 12350.],\n",
       "       [11373., 11085., 10905., ..., 13444., 12350., 11657.],\n",
       "       [11085., 10905., 10930., ..., 12350., 11657., 11358.],\n",
       "       ...,\n",
       "       [16758., 15526., 14313., ..., 18538., 17711., 16938.],\n",
       "       [15526., 14313., 13429., ..., 17711., 16938., 16172.],\n",
       "       [14313., 13429., 13009., ..., 16938., 16172., 15355.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model77.h5')\n",
    "\n",
    "file = open('history77.pkl', 'rb')\n",
    "\n",
    "a = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "average_mae_history = [\n",
    "    np.mean([x[i] for x in all_mae_histories]) for i in range(epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 292ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 997us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 998us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "cnn: [1182.791] 422.0, 640.2, 404.1, 377.6, 509.6, 796.2, 1084.7, 1248.7, 1260.5, 1186.7, 1266.2, 1364.5, 1385.1, 1360.0, 1465.7, 1635.7, 1550.4, 1457.9, 1474.1, 1442.7, 1359.4, 1149.4, 986.8, 1035.5\n"
     ]
    }
   ],
   "source": [
    "score, scores = evaluate_model(train, test, n_input,model)\n",
    "# summarize scores\n",
    "summarize_scores('cnn', score, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
